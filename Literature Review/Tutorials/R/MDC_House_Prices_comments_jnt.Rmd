---
title: "Competition Tutorial"
author: "Mohammed Ali"
date: "October 13, 2018"
output:
  html_document:
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load required libs
load.libraries <- c("tidyverse", "DataExplorer", "corrplot")
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependences = TRUE)
sapply(load.libraries, require, character = TRUE)

#read train and test data
train <- read.csv("../Method Data Science/data/train.csv")
test <- read.csv("../Method Data Science/data/test.csv")
```

# EDA

## Data Structure
first let take us take a glance for `train` structure

```{r structure}
introduce(train)
```

### jnt1: Didn't know the DataExplorer package. Nice!

We can conduct from above the following:
* There is a balance betweeen *discreat* and *continous* features.
* Nearly 6 % of data is missing.
*
Let us see how data is organized.

#### jnt2: Never used glimpse() before. Didn't know this was part of the tidyverse. Nice!
```{r glimbse_summary}
glimpse(train)

summary(train)
```

From above we see:
* There are some outliers scattred here and there, we investigate in details in next sections.
* The missing observaes are scattred among features, let us investigate that more.

## Missing Data

#### jnt3: DataExplorer again!! Very nice plot!

```{r missing_data, fig.height= 20}
plot_missing(train, title = "Missing Data", ggtheme = theme_gray(base_size = 15))
```


The categorical features with the largest number of missing values are: 

* `PoolQC` (99.52%):  Pool Quality, no wonder :)
* `MiscFeature` (96.3%): Miscellaneous features not covered in other categories
* `Alley` (93.7%): indicates the type of alley access
* `Fence` (80%): Fence Quality
* `FirePlaceQu` (47.26%): Fireplace quality
* `GarageType` (5.55%): related features
* `GarageYrBlt` (5.55%): I will convert this feature to categorical and treat it like that
* `GarageFinish` (5.55%): Interior finish of the garage
* `GarageQUal` (5.55%): Garage quality
* `GarageCond` (5.55%): Garage condition
* `BsmtExposure` (2.6%): Refers to walkout or garden level walls.
* `BsmtFinType2` (2.6%): Rating of basement finished area (if multiple types)
* `BsmtQual` (2.53%): Evaluates the height of the basement
* `BsmtCond` (2.53%): Evaluates the general condition of the basement
* `BsmtFinType1` (2.53%): Rating of basement finished area
* `MasVnrType` (0.55%): Masonry veneer type


I will Impute categorical features by converting `NA` to *Not available* level except `MasVnrType` I will add level `others` as they must used something to build with.

The missing values indicate that majority of the houses do not have alley access, no pool, no fence and no elevator, 2nd garage, shed or tennis court that is covered by the MiscFeature.

The numeric variables do not have as many missing values but there are still some present:

* `LotFrontage` (17.74%): Linear feet of street connected to property
    * Maybe I will use the `mean` or `meadin` functions.
    
#### jnt4: I would go for median. It's usually safer.
    
* `MasVnrArea` (0.55%): Masonry veneer area in square feet
    * Will impute by 0, as missing means that it does not exist.

#### jnt5: It does? How do we know?

## Discreate Features Overview

Let us have a quick view

```{r discreate_plots}
plot_bar(train)
```


From the first look there are some features with many levels with no realy small values as:

* `Neighborhood`
* `Condition1`
* `Condition2`
* `HouseStyle`
* `RoofMatl`
* `Exterior1st`
* `Exterior2nd`
* `Functional`
* `SaleType`

## Continuos Features Overview
Now, let us check the continuos features

```{r density}
plot_density(train[,-c(1)], ggtheme = theme_gray(base_size = 15, base_family = "serif"))
```

From plots, it seems there are many fluctations in many features and we will need to deal with each one of it individually.


## Resonse Variable Against Features Overview
 Now, let us see how discreate and continuos features interact with the response variable first.

```{r desnity_response}
plot_scatterplot(train[,-c(1)], by = "SalePrice")
```

The plots confirm my doubs about continuos features in specific, it needs serious handling.
Now let us move to the final stage of our EDA, corrleation.

#### jnt6: What do you mean by serious handling?

## Corrleation

```{r cor_cont}
numeric_var <- names(train)[which(sapply(train, is.numeric))]
correlations <- cor(na.omit(train[, numeric_var]))
# correlations
row_indic <- apply(correlations, 1, function(x) sum(x > 0.3 | x < -0.3) > 1)

correlations<- correlations[row_indic ,row_indic ]
corrplot(correlations, method="square")
```

It seems there is a high corrletation among continuos features, we will need to treat that in Feature Engineering phase.

### Plot scatter plot for variables that have high correlation.

The correlation matrix below shows that there are several variables that are strongly and positively correlated with housing price.

High positive correlation:

* OverallQual
* YearBuilt
* YearRemodAdd
* MasvnrArea
* BsmtFinSF1
* TotalBsmtSF
* 1stFlrSF
* GrLiveArea
* FullBath
* TotRmsAbvGrd
* FirePlaces
* GarageYrBlt
* GarageCars
* GarageArea
* WoodDeskSF
* OpenPorchSF

The number of enclosed porches are negatively correlated with year built. It seems that potential housebuyers do not want an enclosed porch and house developers have been building less enclosed porches in recent years. It is also negatively correlated with SalePrice, which makes sense.

There is some slight negative correlation between OverallCond and SalePrice. There is also strong negative correlation between Yearbuilt and OverallCond. It seems to be that recently built houses tend to been in worse Overall Condition.

```{r OverallCond_YearBuilt}
train %>% 
  select(OverallCond, YearBuilt) %>% 
  ggplot(aes(as.factor(OverallCond),YearBuilt)) +
  geom_boxplot() +
  xlab('Overall Condition')
```

#### Regarding FE (I could not find your Rmd file with this part):

#### jnt7: outlier check: we can remove only the identified outlier or at least standardize all the variables so that our model is less affected by outliers

#### jnt8: missing features: I agree that the best approach is to drop features with very high % of missing values and impute the rest

#### jnt9: identify important features: totally agree that we must find the most important features and work with those; still I have no idea how Boruta works...


